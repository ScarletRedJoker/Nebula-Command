# Nebula Command

## Overview
Nebula Command is a creation engine designed for homelab management, AI-powered content creation, Discord community integration, and multi-platform streaming. It provides a comprehensive, integrated solution for managing homelabs, fostering online communities, and streamlining content creation and distribution, optimized for distributed deployment across cloud and local infrastructure. Key capabilities include a web-based dashboard, a Discord bot, a Stream bot, and hybrid AI services.

## User Preferences
- **Development Workflow:** Edit in Replit → Push to GitHub → Pull on servers
- **Database:** Shared PostgreSQL (Neon in dev, self-hosted in prod)
- **Secrets:** .env file (never commit), Replit Secrets in dev

## System Architecture

### Core Services
The platform is built around three distributed core services:
*   **Dashboard (Next.js 14):** A web interface for homelab management with real-time statistics, Docker controls, SSH server metrics, deployment pipelines, a Monaco code editor, a visual website designer, and an OpenAI-powered AI chat assistant (Jarvis AI).
*   **Discord Bot (Node.js/React):** A highly customizable bot for community management with per-server identity and granular feature toggles.
*   **Stream Bot (Node.js/React/Vite):** Manages multi-platform content posting and interaction across Twitch, YouTube, and Kick.

### Cross-Service Integration
Services share a PostgreSQL database and Redis for caching. Communication occurs via webhooks (Stream Bot to Discord) and APIs (Dashboard and Discord Bot).

### Deployment Architecture
The system utilizes a distributed deployment model across cloud (Linode), Ubuntu Homelab, and Windows AI Node. Tailscale provides secure mesh networking.

### Database Schema
A shared PostgreSQL instance organizes data into distinct databases for each core service: `homelab_jarvis`, `discord_bot`, and `stream_bot`.

### Platform Architecture Expansion
The system features a three-layer design (Experience, Control Plane, Execution Plane) with an event-driven spine. It includes a Marketplace API for Docker packages, an Agent Orchestrator API for AI agents with function calling, and Service Discovery via a `service-map.yml`. A Creative Studio offers AI-powered content creation with multi-model support.

### Universal Creation Engine
The platform includes a Quick Start Wizard, Universal Builder, App Factory (AI-powered code generation), AI Code Assistant, Deploy Pipelines, Template Marketplace, and a Project Manager.

### Auto-Deployment System
Features a Server Provisioning API and a Deployment Execution API for Docker/PM2 deployments, including preflight checks, auto-secret generation, Authelia setup, health checks, Cloudflare DNS sync, smoke tests, and continuous monitoring.

### AI Gateway Architecture
Features a unified AI Chat interface with provider/model selection, real-time streaming responses, and a circuit breaker for fallback. Local AI services (Ollama, Stable Diffusion, ComfyUI) are automatically discovered and configured via Tailscale.

### Discord Bot Architecture
Includes a per-server customization system for bot identity and feature toggles.

### Production Configuration
The dashboard automatically detects development (Replit) or production (Linode) environments, adjusting AI access and SSH key handling. Jarvis AI has autonomous tools using OpenAI function calling. Local AI services on Windows VM and Ubuntu are utilized when running in production via Tailscale. An `/infrastructure` page provides remote control for VMs and Windows services.

### Local AI Services (Windows VM)
A Windows VM hosts GPU-accelerated AI services: Ollama (LLM inference), Stable Diffusion WebUI (image generation), and ComfyUI (node-based video/image workflows). An Auto-Deployment System provides unified management, dependency handling, and a Windows AI Supervisor for service management.

### AI Node Management System
A comprehensive dashboard page (`/ai-nodes`) provides (Agent Port: 9765 via Tailscale):
- **Service Health Monitoring:** Real-time status for Ollama (11434), Stable Diffusion (7860), ComfyUI (8188), and Whisper (8765)
- **GPU Statistics:** Memory usage, utilization, and temperature from nvidia-smi
- **Issue Detection:** Automatic detection of known problems including NumPy 2.x incompatibility, torch.library custom_op errors, xformers mismatches, triton missing, protobuf conflicts, and comfy_kitchen issues
- **Package Version Tracking:** Current vs target versions for numpy (1.26.4), torch (2.3.1), protobuf (5.28.3), xformers (0.0.28), transformers, diffusers
- **One-Click Repair:** Dashboard buttons to restart services, update dependencies, fix specific issues, or repair all
- **Health Daemon:** Windows PowerShell script (`deploy/windows/scripts/start-health-daemon.ps1`) runs continuously, scanning logs for error patterns and reporting package versions every 30 seconds (deep scans every 5 minutes)
- **API Endpoint:** `/api/ai/node-manager` with GET for diagnostics and POST for repair actions
- **Agent Communication:** Requires `NEBULA_AGENT_TOKEN` secret for secure communication with Windows VM agent via Tailscale (100.118.44.102)

### Unified Windows AI Stack Startup
The `deploy/windows/scripts/Start-NebulaAiStack.ps1` script provides one-command startup for all Windows AI services:
- **Python Validation:** Requires Python 3.10-3.12 (rejects 3.14+ as PyTorch lacks CUDA wheels)
- **PyTorch CUDA Repair:** Automatically detects CPU-only PyTorch and reinstalls with CUDA 12.1
- **Ordered Service Start:** Ollama → Stable Diffusion → ComfyUI → Nebula Agent
- **Auto-Start on Boot:** `.\Start-NebulaAiStack.ps1 install` registers as Windows scheduled task
- **Commands:** `start`, `stop`, `status`, `repair`, `validate`, `install`
- **Known Issue Fix:** "Torch not compiled with CUDA enabled" error caused by wrong PyTorch build or Python 3.14

### AI Services API Endpoints
Provides APIs for Speech Services (TTS/STT), Job Scheduling (GPU jobs), Training (LoRA, QLoRA, etc.), and Embeddings/RAG (semantic search, chunking). Core AI libraries support GPU-aware scheduling, hybrid speech services, RAG, and model training.

### Model Management System
A unified model management system via the dashboard and a Windows Model Agent provides model inventory, download management from Civitai/HuggingFace, VRAM estimates, and one-click deletion.

### Model Registry & Catalog Browsing
The Model Registry (`lib/model-registry.ts`) provides unified AI model management:
- **Local Model Inventory:** Track all models across providers (SD checkpoints, LoRAs, Ollama, ComfyUI)
- **HuggingFace Integration:** Search models by task type (text-to-image, text-generation), library, author
- **Civitai Integration:** Browse checkpoints, LoRAs, TextualInversions with ratings and downloads
- **Download Queue:** Queue model downloads to Windows VM with progress tracking
- **API Endpoints:** `/api/ai/models` (inventory), `/api/ai/models/catalog` (browse), `/api/ai/models/install` (download)
- **Graceful Degradation:** Works in offline mode when Windows VM agent is unavailable

### Jarvis AI Orchestrator
The Jarvis Orchestrator (`lib/jarvis-orchestrator.ts`) provides multi-agent AI capabilities:
- **Job Queue System:** Async task management with priorities (critical, high, normal, low)
- **Subagent Management:** Spawn specialized AI workers (code, research, automation, creative)
- **Local-First Resource Selection:** Prefers local GPU resources over cloud APIs
- **Progress Tracking:** Real-time job progress with listener callbacks
- **Tools Available:** generate_image, generate_video, docker_action, deploy, get_server_status, search_codebase, check_ai_services, browse_models, install_model
- **Security:** Dangerous tools (run_command, file operations) are disabled pending proper sandboxing

### Cross-Deployment Health Monitoring
The Health Monitor (`lib/health-monitor.ts`) provides comprehensive system health tracking:
- **Deployment Targets:** Windows VM, Linode, Ubuntu homelab, Replit
- **Service Monitoring:** Ollama, Stable Diffusion, ComfyUI, Whisper, PostgreSQL, Redis, Docker
- **Issue Detection:** service_down, high_cpu, high_memory, high_disk, high_gpu_usage, slow_response
- **Configurable Thresholds:** CPU (80%/95%), Memory (85%/95%), GPU (90%/98%), Disk (85%/95%)
- **Issue Tracking:** Acknowledgement, dismissal, auto-fix with runbooks
- **API Endpoint:** `/api/health/check` for comprehensive health checks

### Notification Service
The Notification Service (`lib/notification-service.ts`) provides actionable alerts:
- **Multi-Channel:** In-app notifications, Discord webhook, email (future)
- **Severity Levels:** Critical, warning, info
- **Deduplication:** 5-minute window to prevent notification spam
- **Actionable Buttons:** View, auto-fix, acknowledge, dismiss
- **Auto-Cleanup:** 24-hour expiry, max 100 stored notifications

### Docker Marketplace
A marketplace offers 24+ pre-built Docker packages across 9 categories for one-click deployment to SSH-connected servers.

### Settings System
A comprehensive settings interface provides configuration for AI, servers, integrations (Discord, Twitch, YouTube), and general settings (profile, appearance, notifications), with connection testing.

### Development vs Production
Differences in SSH access, AI service utilization (cloud vs. local), and specific feature availability are managed by environment detection. Replit development uses modelfarm for OpenAI integration.

### Replit Modelfarm Integration
When running in Replit, the dashboard uses the Replit modelfarm for AI services:
- **Detection:** `AI_INTEGRATIONS_OPENAI_BASE_URL` containing "modelfarm"
- **Chat Models:** gpt-4o, gpt-4o-mini, gpt-4.1, gpt-4.1-mini, gpt-5, gpt-5-mini (NOT gpt-3.5-turbo or gpt-4-turbo)
- **Image Model:** `gpt-image-1` (NOT dall-e-3, and no style/quality parameters)
- **Status Detection:** Dashboard returns "connected" status immediately for modelfarm without API probe

### SSH Key Format Requirements
The ssh2 library (v1.17.0) only supports PEM format private keys, NOT OpenSSH format:
- **Supported:** `-----BEGIN RSA PRIVATE KEY-----`, `-----BEGIN PRIVATE KEY-----`, `-----BEGIN EC PRIVATE KEY-----`
- **Not Supported:** `-----BEGIN OPENSSH PRIVATE KEY-----` (modern OpenSSH default since v7.8)
- **Conversion:** Dashboard attempts automatic conversion, or user can run: `ssh-keygen -p -m pem -f ~/.ssh/id_rsa`
- **Alternative:** Generate new key in PEM format: `ssh-keygen -t rsa -m pem -f ~/.ssh/id_rsa_pem`

## External Dependencies
*   **PostgreSQL:** Primary relational database.
*   **Redis:** Caching and session management.
*   **OpenAI API:** AI services.
*   **Discord API (discord.js):** Discord Bot functionality.
*   **Twitch/YouTube/Kick APIs:** Stream Bot integration.
*   **Spotify API:** Music bot features.
*   **Plex API:** "Now Playing" status.
*   **Home Assistant API:** Homelab automation.
*   **Cloudflare API:** DNS management.
*   **Tailscale:** Secure network connectivity.
*   **Caddy:** Reverse proxy.
*   **Ollama:** Local LLM inference.
*   **Stable Diffusion:** Local image generation.